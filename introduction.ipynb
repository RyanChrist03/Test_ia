{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b364808",
   "metadata": {},
   "source": [
    "## introduzione all'eborazione del linguagio naturale (NPL) usando l'apprendimento non supervisato\n",
    "\n",
    "### Cos' é NPL ?\n",
    "\n",
    "NPL d'all inglese Natural Language Processing, rappresenta l'insieme di metodi e tecniche che permettono ai computer di capire, interpretare e generare un linguagio simile a quello umano.\n",
    "\n",
    "Per qualsiasi progetto, è importante sceglier corretamente i dati e di fare una pre-elaboration, prima di usare dei modelli di IA\n",
    "\n",
    "Esiste un processo di pre-elaboratione chiamato __Data cleaning__ ovvero __Pulizia dei dati__\n",
    "\n",
    "### Pre-elaborazion: Data cleaning\n",
    "\n",
    "il Data cleaning si fa i 6 passi:\n",
    "\n",
    "* Eliminazione della puntuazione\n",
    "* Tokenization (tokenizzazione)\n",
    "* Eliminazione delle parole vuote\n",
    "* Stemming\n",
    "* Lemmatisation\n",
    "* Part of Speech Tagging\n",
    "\n",
    "#### Eliminazione della puntuazione\n",
    "\n",
    "La puntuazione non è utile, non aguinge senso alla frase, al linguagio. allora vengono cancelli tutti segni di puntuazione.\n",
    "\n",
    "Esempio con Python:\n",
    "\n",
    "Per inziare occore cambiare il giocco di dati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4a5f062-0e3f-4a7a-af01-9c8075ddf653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf319a0d-d81e-4f9b-8385-d10030cc8e65",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../../data/NLP/Corona_NLP_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../../../data/NLP/Corona_NLP_train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatin1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataframe)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../../data/NLP/Corona_NLP_train.csv'"
     ]
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"../../../../data/NLP/Corona_NLP_train.csv\", encoding='latin1')\n",
    "\n",
    "print(dataframe)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e1707685-145e-4dfd-b462-7e7f416990a3",
   "metadata": {},
   "source": [
    "       UserName  ScreenName                      Location     TweetAt  \\\n",
    "0          3799       48751                        London  16-03-2020   \n",
    "1          3800       48752                            UK  16-03-2020   \n",
    "2          3801       48753                     Vagabonds  16-03-2020   \n",
    "3          3802       48754                           NaN  16-03-2020   \n",
    "4          3803       48755                           NaN  16-03-2020   \n",
    "...         ...         ...                           ...         ...   \n",
    "41152     44951       89903  Wellington City, New Zealand  14-04-2020   \n",
    "41153     44952       89904                           NaN  14-04-2020   \n",
    "41154     44953       89905                           NaN  14-04-2020   \n",
    "41155     44954       89906                           NaN  14-04-2020   \n",
    "41156     44955       89907  i love you so much || he/him  14-04-2020   \n",
    "\n",
    "                                           OriginalTweet           Sentiment  \n",
    "0      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
    "1      advice Talk to your neighbours family to excha...            Positive  \n",
    "2      Coronavirus Australia: Woolworths to give elde...            Positive  \n",
    "3      My food stock is not the only one which is emp...            Positive  \n",
    "4      Me, ready to go at supermarket during the #COV...  Extremely Negative  \n",
    "...                                                  ...                 ...  \n",
    "41152  Airline pilots offering to stock supermarket s...             Neutral  \n",
    "41153  Response to complaint not provided citing COVI...  Extremely Negative  \n",
    "41154  You know itÂ’s getting tough when @KameronWild...            Positive  \n",
    "41155  Is it wrong that the smell of hand sanitizer i...             Neutral  \n",
    "41156  @TartiiCat Well new/used Rift S are going for ...            Negative  \n",
    "\n",
    "[41157 rows x 6 columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef65d2-d241-4b84-afcc-431ac86b4fcf",
   "metadata": {},
   "source": [
    "Cancelleremo la puntuazione nei tweets originals\n",
    "\n",
    "Per quello creeremo una funzione Cancella_Puntuazione che dovra usae dei Regex per la cancellazione. Prima, dobiamo importare la libreria python che gestisce i regx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "351e6054-ef33-4b9a-8e3a-18fc9abdbee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc016f39-8917-4a16-a3cf-651168fd5f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cancella_Puntuazione(testo):\n",
    "    return re.sub(r'[^\\w\\s]', '', testo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3df1291-2af5-487c-8c1c-5683a9ab94e0",
   "metadata": {},
   "source": [
    "andremo poi a uasare questa funzione per cancellare tutti segni di puntuazione nei testi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f811b9-e73b-4820-8cf7-c07832e71538",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m testo_senza_puntuazione \u001b[38;5;241m=\u001b[39m [Cancella_Puntuazione(testo) \u001b[38;5;28;01mfor\u001b[39;00m testo \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataframe\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginalTweet\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(testo_senza_puntuazione[:\u001b[38;5;241m5\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "testo_senza_puntuazione = [Cancella_Puntuazione(testo) for testo in dataframe['OriginalTweet']]\n",
    "print(testo_senza_puntuazione[:5])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffd577c3-eac0-48ff-ad03-43c9fe37481c",
   "metadata": {},
   "source": [
    "['MeNyrbie Phil_Gahan Chrisitv httpstcoiFz9FAn2Pa and httpstcoxX6ghGFzCC and httpstcoI2NlzdxNo8', 'advice Talk to your neighbours family to exchange phone numbers create contact list with phone numbers of neighbours schools employer chemist GP set up online shopping accounts if poss adequate supplies of regular meds but not over order', 'Coronavirus Australia Woolworths to give elderly disabled dedicated shopping hours amid COVID19 outbreak httpstcobInCA9Vp8P', 'My food stock is not the only one which is empty\\r\\r\\n\\r\\r\\nPLEASE dont panic THERE WILL BE ENOUGH FOOD FOR EVERYONE if you do not take more than you need \\r\\r\\nStay calm stay safe\\r\\r\\n\\r\\r\\nCOVID19france COVID_19 COVID19 coronavirus confinement Confinementotal ConfinementGeneral httpstcozrlG0Z520j', 'Me ready to go at supermarket during the COVID19 outbreak\\r\\r\\n\\r\\r\\nNot because Im paranoid but because my food stock is litteraly empty The coronavirus is a serious thing but please dont panic It causes shortage\\r\\r\\n\\r\\r\\nCoronavirusFrance restezchezvous StayAtHome confinement httpstcousmuaLq72n']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0f7a03-3b0b-4f48-b5f7-9c73954461ad",
   "metadata": {},
   "source": [
    "Dopo la cancellazione di tutti segni di puntuazione possiamo passare alla __Tokenization__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a37073-1b4a-44b0-b12b-a919134287d8",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "La tokenizzazione (Tokenization) consente di separare le parole della stringa di caratteri in un elenco di parole separate. Questo sarà molto importante per il trattamento dei nostri dati nei diversi modelli.\n",
    "\n",
    "per fare la tokenizzazione si puo usare divese librerie come NLTK specialisata nell'elaborazione del liguagio Naturale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8529f7a-c732-428b-ad5e-6220349c86d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# dati necissarie pe la tokenizzaziione con nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3210a3-9153-4ce4-b6c3-e0c3c4838d30",
   "metadata": {},
   "source": [
    "Scriviamo la funzione tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec972330-95ed-48da-b6e8-0f0e8d5c496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(testo_senza_Puntuazione):\n",
    "    testo_tokenized = []\n",
    "    for testo in testo_senza_Puntuazione:\n",
    "        testo_tokenized.append(word_tokenize(testo))\n",
    "    return testo_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da4f852-258c-4c1c-8187-8e1ec5503672",
   "metadata": {},
   "outputs": [],
   "source": [
    "testo_tokenized = tokenize(testo_senza_puntuazione)\n",
    "print(testo_tokenized)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44212d99-105b-4b27-b23e-101911f00574",
   "metadata": {},
   "source": [
    "['advice', 'Talk', 'to', 'your', 'neighbours', 'family', 'to', 'exchange', 'phone', 'numbers', 'create', 'contact', 'list', 'with', 'phone', 'numbers', 'of', 'neighbours', 'schools', 'employer', 'chemist', 'GP', 'set', 'up', 'online', 'shopping', 'accounts', 'if', 'poss', 'adequate', 'supplies', 'of', 'regular', 'meds', 'but', 'not', 'over', 'order']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638719a8-913f-4473-813e-8e77597811b2",
   "metadata": {},
   "source": [
    "Dopo la tokenizzazione si cancellera le parole vuote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76d7027-358b-41c4-acef-72299d722485",
   "metadata": {},
   "source": [
    "#### Eliminazione delle parole vuote\n",
    "\n",
    "L' elimonazione delle parole vuote permette di togiliere le parole per la struttura del linguagio ( detto Liguagio strutturante), ma che non contribuisce al contenuto o  alla suo comprensione\n",
    "\n",
    "Ad esempio: il, la, un, una ...\n",
    "\n",
    "\n",
    "per farlo possiamo usare l'elenco dei stop_words della libreria nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff058a4-a947-44a2-a256-3d7f2c491de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# i dati per aver un'elenco di stop_words\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b993a7cc-28c1-4dae-8b4a-34b7f027e29d",
   "metadata": {},
   "source": [
    "Poi definiremo la nostra funzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d5ecc7-8966-4724-9c1f-a695100775e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cancellazione_ParoleVuote(testo_tokenized):\n",
    "    testo_senza_parolevuote = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for testo in testo_tokenized:\n",
    "        testo_senza_parolevuote.append([parola for parola in testo if parola not in stop_words])\n",
    "    return testo_senza_parolevuote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10484c75-11fa-456d-9867-bfb1ed61b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testo_senza_parolevuote = Cancellazione_parolevuote(testo_tokenized)\n",
    "\n",
    "print(testo_senza_parolevuote)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4364cdf7-21c5-48d3-a2ff-35e346d02247",
   "metadata": {},
   "source": [
    "['advice', 'Talk', 'neighbours', 'family', 'exchange', 'phone', 'numbers', 'create', 'contact', 'list', 'phone', 'numbers', 'neighbours', 'schools', 'employer', 'chemist', 'GP', 'set', 'online', 'shopping', 'accounts', 'poss', 'adequate', 'supplies', 'regular', 'meds', 'order']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51be4e04-c46e-454b-acfb-b6d9b84cb246",
   "metadata": {},
   "source": [
    "Dopo la cancellazione delle parole vuote possiamo realizare una pre-elaborazione permettente di mettere tutte le parole in minuscolo\n",
    "\n",
    "Ecco una funzione per farlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf1e2a8-546c-4862-bc02-b1c354d373be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Metti_in_minuscolo(testo_senza_parolevuote):\n",
    "    testo_minuscolo = []\n",
    "    for testo in testo_senza_parolevuote:\n",
    "        testo_minuscolo.append([parola.lower() for parola in testo])\n",
    "    return testo_minuscolo"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b40e172-4895-4f94-b440-3b2c34f75f95",
   "metadata": {},
   "source": [
    "['advice', 'talk', 'neighbours', 'family', 'exchange', 'phone', 'numbers', 'create', 'contact', 'list', 'phone', 'numbers', 'neighbours', 'schools', 'employer', 'chemist', 'gp', 'set', 'online', 'shopping', 'accounts', 'poss', 'adequate', 'supplies', 'regular', 'meds', 'order']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d699739b-bf71-46fc-88a1-c39a6af5c26f",
   "metadata": {},
   "source": [
    " #### Stemming\n",
    "\n",
    " Lo stemming è il processo di reduzione della forma flessa di una parola alla sua forma radice detta anche \"tema\". le parole non honna necessaraimente un significato proprio nella nostra lingua\n",
    "\n",
    " Ecco una funzione per realizarlo, sempre con nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a3b477-3099-471d-b00c-d346e59d8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45f23fb-d150-49e9-a66d-b9a7ed901a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stemming( testo_minuscolo):\n",
    "    testo_stemming = []\n",
    "    for testo in testo_minuscolo:\n",
    "        testo_stemming.append([PorterStemmer().stem(parola) for parola in testo])\n",
    "    return testo_stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ccf43-00ef-4287-8686-3093e96336fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "testo_stemming = Stemming(testo_minuscolo)\n",
    "\n",
    "print(testo_stemming)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64b35fdd-2e66-4cb9-8a70-58258d79a5ef",
   "metadata": {},
   "source": [
    "['advic', 'talk', 'neighbour', 'famili', 'exchang', 'phone', 'number', 'creat', 'contact', 'list', 'phone', 'number', 'neighbour', 'school', 'employ', 'chemist', 'gp', 'set', 'onlin', 'shop', 'account', 'poss', 'adequ', 'suppli', 'regular', 'med', 'order']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f779efb-b088-4ee6-b6cf-43230f5b3ff2",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "La Lemmatizzazione ( Lemmentization in inglese) è il processo algoritmico  che determina automaticamente il __lemma__ ( forma canonica) di una parola.\n",
    "la forma cononica è la forma di rifermento per cercare la parola all'interno di un dizionario.\n",
    "\n",
    "\n",
    "*è preferabile fare una lemmatizzazion invece di uno stemming quando il testo fornito è abbastanza puilito o contiene meno errori\n",
    "esempio, lo stemming non fara mai la differenza tra le parole \"est\" e \"Grand-est\"*\n",
    "\n",
    "*il testo deve quindi essere pulito, altrminmento se essistono errori di ortografa il seginifica della parola puo essere cambiata e mal interprettata dopo la lemmatizzazione gli errori possonno anche essere grammatica o di sintasse o altri*\n",
    "\n",
    "⚠️ La lemmatizzazione deve essere fata prima del retiro delle stop_words!\n",
    "Se vogliammo che la __Lemmatizzazione__ sia meglio dello __Stemming__ dobbiamo fare il __POS tagging__ .\n",
    "*In questa sezion esporremo solo  comme implementare le tecniche di cui avremo bisogno in futuro per pre-elaborare i dati*\n",
    "\n",
    "Ecco una funzione per realizzare la __Lemmatizzazione__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b52675-3357-40e5-ad71-5e9b2ebda012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d960d1-cc9c-47fc-9a3a-3d3e930e3f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lemmatizzazione(testo_minuscolo):\n",
    "    testo_lemmatized = []\n",
    "    for testo in testo_minuscolo:\n",
    "        testo_lemmatized.append([WordNetLemmatizer().lemmatize(parola) in testo])\n",
    "    return testo_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37159861-be29-42d1-94b9-85100d68c80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testo_lemmatized = Lemmatizzazione(testo_minuscolo)\n",
    "\n",
    "print(testo_lemmatized)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cebf8fea-b4d1-4e43-a64d-6c41402c21bd",
   "metadata": {},
   "source": [
    "['advice', 'talk', 'neighbour', 'family', 'exchange', 'phone', 'number', 'create', 'contact', 'list', 'phone', 'number', 'neighbour', 'school', 'employer', 'chemist', 'gp', 'set', 'online', 'shopping', 'account', 'po', 'adequate', 'supply', 'regular', 'med', 'order']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c94b3d-64d2-4b03-9e62-506af12eccd6",
   "metadata": {},
   "source": [
    "#### Part of Speech Tagging ( POS tagging)\n",
    "\n",
    "Il __part of speech tagging__ ovvero l'analisi gammaticale di testo è uno dei task piu classici dell' __NPL__ che consiste all'attribuzione ad ogni parola della relativa carrateristica lessicale (Nome, verbo, pronome etc..)\n",
    "\n",
    "In quasi tutte le funzione che permettono le Lemmatizzazione, il POS Tagging si fa sempre.Tuttavia nella libreria nltk con la funzione utilizzata il POS Tagging non vienne realizzato. vi invito a leggere la documentazione delle librerie che volete usare per la pre_elaborazione.\n",
    "\n",
    "Il POS Tagging è fatto dopo la fase de Tokenizzazione, quindi ricomminceremo la su.\n",
    "\n",
    "Realizzazione del __POS Tagging__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e908ba-cc87-4ead-a62b-69b2b78e2688",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c99fb9-daed-4edd-97fa-8498c125b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pos_tagging(testo):\n",
    "    testo_tagged = []\n",
    "    for text in testo:\n",
    "        testo_tagged.append(nltk.pos_tag(testo))\n",
    "    return testo_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41f51a5-be9b-447a-ac65-89ae634c6043",
   "metadata": {},
   "outputs": [],
   "source": [
    "testo_tagged =  Pos_tagging(testo_tokenized)\n",
    "\n",
    "print(testo_tagged[1])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f17b9ea-857a-40ea-9f46-e2da535e51c3",
   "metadata": {},
   "source": [
    "[('advice', 'NN'), ('Talk', 'NN'), ('to', 'TO'), ('your', 'PRP$'), ('neighbours', 'NNS'), ('family', 'NN'), ('to', 'TO'), ('exchange', 'VB'), ('phone', 'NN'), ('numbers', 'NNS'), ('create', 'VBP'), ('contact', 'JJ'), ('list', 'NN'), ('with', 'IN'), ('phone', 'NN'), ('numbers', 'NNS'), ('of', 'IN'), ('neighbours', 'NNS'), ('schools', 'NNS'), ('employer', 'VBP'), ('chemist', 'JJ'), ('GP', 'NNP'), ('set', 'VBD'), ('up', 'RP'), ('online', 'JJ'), ('shopping', 'NN'), ('accounts', 'NNS'), ('if', 'IN'), ('poss', 'JJ'), ('adequate', 'JJ'), ('supplies', 'NNS'), ('of', 'IN'), ('regular', 'JJ'), ('meds', 'NNS'), ('but', 'CC'), ('not', 'RB'), ('over', 'IN'), ('order', 'NN')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84afb1e2-8a98-48e4-accc-fc171f831085",
   "metadata": {},
   "source": [
    "Di norma è dopo questa fase che vienne realizzata la __Lemmatizzazione__. in questo cas dobbiamo implementare un' altra funzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1a9bd-15c2-4e64-a5ff-57cf9d4cd297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pos_taggin_lemmatizzazione(testo):\n",
    "    testo_lemmatized = []\n",
    "    for text in testo:\n",
    "        lemma_tags = []\n",
    "        for token, tag in testo:\n",
    "            if tag.startswith('N'):\n",
    "               lemma = WordNetLemmatizer().lemmatize(token, pos='n')\n",
    "            elif tag.startswith('V'):\n",
    "                lemma = WordNetLemmatizer().lemmatize(token, pos='v')\n",
    "            elif tag.startswith('J'):\n",
    "                lemma = WordNetLemmatizer().lemmatize(token, pos='a')\n",
    "            elif tag.startswith('R'):\n",
    "                lemma = WordNetLemmatizer().lemmatize(token, pos='r')\n",
    "            else:\n",
    "                lemma = WordNetLemmatizer().lemmatize(token)\n",
    "            lemma_tags.append(lemma)\n",
    "        testo_lemmatized.append(lemma_tags)\n",
    "    return testo_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce54ae97-8bfb-4584-aa27-6720c7249938",
   "metadata": {},
   "outputs": [],
   "source": [
    "testo_lemmatized = Pos_taggin_lemmatizzazione(testo_tag)\n",
    "\n",
    "print(testo_lemmatized[1])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "403ba7a9-7ba5-423f-8c66-54dd11386056",
   "metadata": {},
   "source": [
    "['advice', 'Talk', 'to', 'your', 'neighbour', 'family', 'to', 'exchange', 'phone', 'number', 'create', 'contact', 'list', 'with', 'phone', 'number', 'of', 'neighbour', 'school', 'employer', 'chemist', 'GP', 'set', 'up', 'online', 'shopping', 'account', 'if', 'poss', 'adequate', 'supply', 'of', 'regular', 'med', 'but', 'not', 'over', 'order']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0839aaeb-954d-4eb1-b151-9e921d676ed8",
   "metadata": {},
   "source": [
    "### In breve\n",
    "\n",
    "per realizzare la __lemmatizzazione dobbiamo seguire un certo ordine di passagi:\n",
    "\n",
    "__1° caso__\n",
    "\n",
    "Tokenizzazione --> POS Tagging ( se non incluso nella Lemmatizzazione) --> Lemmatizzazione --> Eliminazione della Puntuazione --> Casting ( minuscolo o maiuscolo) --> Eliminazione dei stop-words\n",
    "\n",
    "__2° caso__\n",
    "Cancellazione della puntuazione --> Tokenizzazione --> Casting --> Cnacellazione dei stop-words --> Stemming\n",
    "\n",
    "\n",
    "### Concluzione\n",
    "\n",
    "Questo tipo di per-elaborazione di dati riguarda il text-mining, vedere il corso sul __Text-mining__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
